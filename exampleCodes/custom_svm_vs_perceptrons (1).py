# -*- coding: utf-8 -*-
"""Custom SVM vs Perceptrons.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-jxbt7Z2bw-TWwRcqqLgKkyFhnfiR7O1

### **Implementation of custom SVM vs Custom Perceptrons**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

# Generate a simple dataset using make_blobs
X, y = make_blobs(n_samples=100, centers=2, random_state=42)
y = np.where(y == 0, -1, 1)  # Convert labels to -1 and 1

# Normalize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a column of ones to X for the intercept term
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Perceptron Class
class Perceptron:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.w = None

    def fit(self, X, y):
        m, n = X.shape
        self.w = np.zeros(n)
        for _ in range(self.epochs):
            for i in range(m):
                if y[i] * (X[i] @ self.w) <= 0:
                    self.w += self.learning_rate * y[i] * X[i]

    def predict(self, X):
        return np.sign(X @ self.w)

# SVM Class
class SVM:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.w = None

    def fit(self, X, y):
        m, n = X.shape
        self.w = np.zeros(n)
        for _ in range(self.epochs):
            for i in range(m):
                if y[i] * (X[i] @ self.w) < 1:
                    self.w += self.learning_rate * (y[i] * X[i])
                else:
                    self.w -= self.learning_rate * (0.01 * self.w)  # Regularization term

    def predict(self, X):
        return np.sign(X @ self.w)

# Train Perceptron
perceptron = Perceptron(learning_rate=0.01, epochs=1000)
perceptron.fit(X, y)
perceptron_predictions = perceptron.predict(X)

# Train SVM
svm = SVM(learning_rate=0.01, epochs=1000)
svm.fit(X, y)
svm_predictions = svm.predict(X)

# Plotting function
def plot_decision_boundary(X, y, model, title):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 1], X[:, 2], c=y, cmap='bwr', alpha=0.7)
    x_min, x_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    y_min, y_max = X[:, 2].min() - 1, X[:, 2].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = np.c_[np.ones(xx.ravel().shape), xx.ravel(), yy.ravel()] @ model.w
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf], colors=['#FFAAAA', '#AAAAFF'], alpha=0.3)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

# Plot decision boundaries
plot_decision_boundary(X, y, perceptron, 'Perceptron Decision Boundary')
plot_decision_boundary(X, y, svm, 'SVM Decision Boundary')

# Print the weights
print("Weights from Custom Perceptron :", perceptron.w)
print("Weights from Custom SVM :", svm.w)

"""### ***Implementation of sub gradient algorithm***"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

# Generate a simple dataset using make_blobs
X, y = make_blobs(n_samples=100, centers=2, random_state=42)
y = np.where(y == 0, -1, 1)  # Convert labels to -1 and 1

# Normalize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a column of ones to X for the intercept term
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Perceptron Loss Function
def perceptron_loss(w, X, y):
    return np.maximum(0, -y * (X @ w)).mean()

# Hinge Loss Function
def hinge_loss(w, X, y):
    return np.maximum(0, 1 - y * (X @ w)).mean()

# Subgradient Descent Algorithm
def subgradient_descent(X, y, loss_func, learning_rate=0.001, epochs=1000):
    m, n = X.shape
    w = np.zeros(n)
    for epoch in range(epochs):
        if loss_func == perceptron_loss:
            mask = y * (X @ w) <= 0
            subgrad = -(y[mask][:, np.newaxis] * X[mask]).mean(axis=0) if mask.any() else np.zeros(n)
        elif loss_func == hinge_loss:
            mask = y * (X @ w) < 1
            subgrad = -(y[mask][:, np.newaxis] * X[mask]).mean(axis=0) if mask.any() else np.zeros(n)
        w -= learning_rate * subgrad
    return w

# Optimize Perceptron Loss
w_perceptron = subgradient_descent(X, y, perceptron_loss, learning_rate=0.001, epochs=1000)

# Optimize Hinge Loss
w_hinge = subgradient_descent(X, y, hinge_loss, learning_rate=0.001, epochs=1000)

# Plotting function
def plot_decision_boundary(X, y, w, title):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 1], X[:, 2], c=y, cmap='bwr', alpha=0.7)
    x_min, x_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    y_min, y_max = X[:, 2].min() - 1, X[:, 2].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = np.c_[np.ones(xx.ravel().shape), xx.ravel(), yy.ravel()] @ w
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf], colors=['#FFAAAA', '#AAAAFF'], alpha=0.3)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

# Plot decision boundaries
plot_decision_boundary(X, y, w_perceptron, 'Perceptron Loss Decision Boundary')
plot_decision_boundary(X, y, w_hinge, 'Hinge Loss Decision Boundary')

# Print the weights
print("Weights from Perceptron Loss Optimization:", w_perceptron)
print("Weights from Hinge Loss Optimization:", w_hinge)

"""### ***Comparing the custom implementation to the optimized implementation of Perceptron and SVM.***"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Perceptron
from sklearn.svm import LinearSVC

# Generate a simple dataset using make_blobs
X, y = make_blobs(n_samples=100, centers=2, random_state=42)
y = np.where(y == 0, -1, 1)  # Convert labels to -1 and 1

# Normalize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a column of ones to X for the intercept term
X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))

# Perceptron Loss Function
def perceptron_loss(w, X, y):
    return np.maximum(0, -y * (X @ w)).mean()

# Hinge Loss Function
def hinge_loss(w, X, y):
    return np.maximum(0, 1 - y * (X @ w)).mean()

# Subgradient Descent Algorithm
def subgradient_descent(X, y, loss_func, learning_rate=0.001, epochs=1000):
    m, n = X.shape
    w = np.zeros(n)
    for epoch in range(epochs):
        if loss_func == perceptron_loss:
            mask = y * (X @ w) <= 0
            subgrad = -(y[mask][:, np.newaxis] * X[mask]).mean(axis=0) if mask.any() else np.zeros(n)
        elif loss_func == hinge_loss:
            mask = y * (X @ w) < 1
            subgrad = -(y[mask][:, np.newaxis] * X[mask]).mean(axis=0) if mask.any() else np.zeros(n)
        w -= learning_rate * subgrad
    return w

# Optimize Perceptron Loss
w_perceptron_scratch = subgradient_descent(X_with_intercept, y, perceptron_loss, learning_rate=0.001, epochs=1000)

# Optimize Hinge Loss
w_hinge_scratch = subgradient_descent(X_with_intercept, y, hinge_loss, learning_rate=0.001, epochs=1000)

# Train Perceptron using scikit-learn
perceptron_model = Perceptron()
perceptron_model.fit(X, y)
w_perceptron_sklearn = np.hstack((perceptron_model.intercept_, perceptron_model.coef_.flatten()))

# Train SVM using scikit-learn
svm_model = LinearSVC(loss='hinge', max_iter=1000)
svm_model.fit(X, y)
w_hinge_sklearn = np.hstack((svm_model.intercept_, svm_model.coef_.flatten()))

# Create a comparison table
import pandas as pd

comparison_table = pd.DataFrame({
    'Weights': ['Intercept', 'Feature 1', 'Feature 2'],
    'Perceptron (Scratch)': w_perceptron_scratch,
    'Perceptron (sklearn)': w_perceptron_sklearn,
    'SVM (Scratch)': w_hinge_scratch,
    'SVM (sklearn)': w_hinge_sklearn
})

print(comparison_table)