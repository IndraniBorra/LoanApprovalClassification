# -*- coding: utf-8 -*-
"""Regression_Linear.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V-ddzsusgkl5rlrMKhP2e_uIskpOorwj

### ***Custom Linear Regression vs Linear regression with sklearn***
"""

import numpy as np
import pandas as pd
from urllib.request import urlopen
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# Load the dataset
url = "https://lib.stat.cmu.edu/datasets/boston"
raw_data = urlopen(url).read().decode('utf-8').splitlines()

# Parse the dataset
data = []
for line in raw_data[22:]:
    data.extend([float(x) for x in line.split()])

# Convert to numpy array and reshape
data = np.array(data).reshape(-1, 14)

# Split into features and target
X = data[:, :-1]
y = data[:, -1]

# Normalize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Add a column of ones to X for the intercept term
X = np.hstack((np.ones((X.shape[0], 1)), X))

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression Implementation from Scratch


# Function to compute Mean Squared Error
def compute_mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)


# Function to compute Mean Absolute Error
def compute_mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# Function for gradient descent
def gradient_descent(X, y, learning_rate=0.01, epochs=500):
    m, n = X.shape  # Get the shape of the data
    theta = np.zeros(n)
    for epoch in range(epochs):
        y_pred = np.dot(X,theta)          #y_pred = X @ theta
        error = y_pred - y    #calculate the error
        gradient = (2/m) * np.dot(X.T, error)         #gradient = (2/m) * (X.T @ error)
        theta -= learning_rate * gradient   #update the weights
    return theta

# Train the model using gradient descent
theta = gradient_descent(X_train, y_train, learning_rate=0.01, epochs=500)

# Make predictions
y_train_pred = np.dot(X_train,theta) #predictions on training data
y_test_pred = np.dot(X_test,theta)  #predictions on testing data

# Compute errors for the custom implementation
mse_train_custom = compute_mse(y_train, y_train_pred)
mae_train_custom = compute_mae(y_train, y_train_pred)
mse_test_custom = compute_mse(y_test, y_test_pred)
mae_test_custom = compute_mae(y_test, y_test_pred)

# Linear Regression using sklearn
model = LinearRegression()  # Create a LinearRegression object
model.fit(X_train, y_train) # Train the model on training data
y_train_pred_sklearn = model.predict(X_train) # Predictions on training data
y_test_pred_sklearn = model.predict(X_test) # Predictions on testing data

# Compute errors for the sklearn implementation
mse_train_sklearn = mean_squared_error(y_train, y_train_pred_sklearn)
mae_train_sklearn = mean_absolute_error(y_train, y_train_pred_sklearn)
mse_test_sklearn = mean_squared_error(y_test, y_test_pred_sklearn)
mae_test_sklearn = mean_absolute_error(y_test, y_test_pred_sklearn)

# Print the results
print("Custom Implementation:")
print(f"Training MSE: {mse_train_custom}")
print(f"Training MAE: {mae_train_custom}")
print(f"Testing MSE: {mse_test_custom}")
print(f"Testing MAE: {mae_test_custom}")

print("\nSklearn Implementation:")
print(f"Training MSE: {mse_train_sklearn}")
print(f"Training MAE: {mae_train_sklearn}")
print(f"Testing MSE: {mse_test_sklearn}")
print(f"Testing MAE: {mae_test_sklearn}")

# Compare results in a table
results = pd.DataFrame({
    "Metric": ["MSE", "MAE"],
    "Train (Custom)": [mse_train_custom, mae_train_custom],
    "Test (Custom)": [mse_test_custom, mae_test_custom],
    "Train (Sklearn)": [mse_train_sklearn, mae_train_sklearn],
    "Test (Sklearn)": [mse_test_sklearn, mae_test_sklearn]
})

print("\nComparison Table:")
print(results)